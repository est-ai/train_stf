{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "#!pip install torchinfo\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'\n",
    "\n",
    "from src import autoencoder as ae\n",
    "from torchinfo import summary\n",
    "\n",
    "from glob import glob, escape\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from src.datagen_aug import datagen, LipGanDS\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "datas = {'data_root/val':1}\n",
    "USE_TANH = True\n",
    "device = 'cuda'\n",
    "\n",
    "from moviepy import editor as mpe\n",
    "import IPython.display as ipd\n",
    "\n",
    "\n",
    "class args:\n",
    "    mel_ps = 80 \n",
    "    img_size = 288\n",
    "    batch_size = 4 \n",
    "    mel_step_size = 108\n",
    "    mask_ver =(9) \n",
    "    mask_img_trsf_ver = 0\n",
    "    num_ips = 2\n",
    "    # lr = 5e-3\n",
    "    num_workers = 4\n",
    "    mel_trsf_ver = -1\n",
    "    \n",
    "def load_model(weight_path):\n",
    "    model = ae.Speech2Face(3, (3, args.img_size, args.img_size), (1, 96, args.mel_step_size))\n",
    "    model.load_state_dict(torch.load(weight_path, map_location=device))\n",
    "    return model.to(device).eval()\n",
    "\n",
    "        \n",
    "def get_dl():\n",
    "        \n",
    "    args.val_images = []\n",
    "    for data_root_val, stride in datas.items():\n",
    "        args.val_images += sorted(glob(f'{data_root_val}/*/*.jpg'))[:-60:stride]\n",
    "    for data_root_val, stride in datas.items():\n",
    "        args.val_images += sorted(glob(f'{data_root_val}/*/*.jpg'))[60::stride]\n",
    "    print('len(val_images):', len(args.val_images))\n",
    "    ds = LipGanDS(args, phase='val')\n",
    "    ds[0]\n",
    "    batch_size = args.batch_size\n",
    "    dl = DataLoader(dataset=ds, batch_size=batch_size, num_workers=args.num_workers, shuffle=False, drop_last=True)\n",
    "    return dl\n",
    "dl = get_dl()\n",
    "#for _ in dl:\n",
    "#    pass\n",
    "len(dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-conservative",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds=dl.dataset\n",
    "#img_gt, mel, ips = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from PIL import Image\n",
    "##img_gt_n = to_img(img_gt)\n",
    "##display(Image.fromarray(img_gt_n[0][:,:,::-1]))\n",
    "##t = to_img(ips[0][:,:,0:3])\n",
    "##display(Image.fromarray(t[:,:,::-1]))\n",
    "#\n",
    "#\n",
    "#frames = []        \n",
    "#for img_gt, mel, ips  in tqdm(dl):\n",
    "#    \n",
    "#    audio = mel.unsqueeze(1).to(device)\n",
    "#    ips = ips.to(device).permute(0,3,1,2)\n",
    "#    ips = ips.permute(0,2,3,1)\n",
    "#    b,h,w,c = ips.shape\n",
    "#    print('ips.shape', ips.shape)\n",
    "#    ipss = [e.reshape(h,w,3,c//3).permute(2,0,1,3) for e in ips]\n",
    "#    print('ipss[0].shape', ipss[0].shape)\n",
    "#    ips2 = torch.stack(ipss, axis=1)\n",
    "#    print('ips2.shape', ips.shape)\n",
    "#    img_gt, ips3 = to_img(img_gt), [to_img(e) for e in ips2]\n",
    "#    print('len(ips3):', len(ips3), 'ips3[0].shape', ips3[0].shape)\n",
    "#    \n",
    "#    frames.append(np.concatenate(list(reversed(ips3 + [img_gt])), axis=2))\n",
    "#    break\n",
    "#print(frames[0].shape)\n",
    "##frames = np.concatenate(frames, axis=0)[:,:,:,[2,1,0]]\n",
    "##frames[0].shape\n",
    "##Image.fromarray(frames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frames2 = np.concatenate(frames, axis=0)[:,:,:,[2,1,0]]\n",
    "#print(frames2.shape)\n",
    "#Image.fromarray(frames2[0])\n",
    "#for i in range(3):\n",
    "#    t = to_img(ips[0][:,:,i*3:i*3+3])\n",
    "#    print(t.shape)\n",
    "#    display(Image.fromarray(t[:,:,::-1]))\n",
    "#for im in ipss[0]:\n",
    "#    t = to_img(im)\n",
    "#    print(t.shape)\n",
    "#    display(Image.fromarray(t[:,:,::-1]))\n",
    "#    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_img(t):\n",
    "    img = t.cpu().numpy().astype(np.float64)\n",
    "    img = ((img / 2.0) + 0.5) * 255.0\n",
    "    img = np.clip(img, 0.0, 255.0).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "def get_generated_frame(model, dl, device):\n",
    "    frames = []\n",
    "    model.eval()\n",
    "        \n",
    "    for img_gt, mel, ips  in tqdm(dl):\n",
    "        \n",
    "        audio = mel.unsqueeze(1).to(device)\n",
    "        ips = ips.to(device).permute(0,3,1,2)\n",
    "       \n",
    "        with torch.no_grad():\n",
    "            img_pred = model(ips, audio)\n",
    "            \n",
    "        gen_face  = to_img(img_pred.permute(0,2,3,1))\n",
    "        ips = ips.permute(0,2,3,1)\n",
    "        b,h,w,c = ips.shape\n",
    "        #print('ips.shape', ips.shape)\n",
    "        ipss = [e.reshape(h,w,3,c//3).permute(2,0,1,3) for e in ips]\n",
    "        #print('ipss[0].shape', ipss[0].shape)\n",
    "        ips = torch.stack(ipss, axis=1)\n",
    "        #print('ips.shape', ips.shape)\n",
    "        img_gt, ips = to_img(img_gt), [to_img(e) for e in ips][::-1]\n",
    "        \n",
    "        frames.append(np.concatenate(list(reversed(ips + [gen_face, img_gt])), axis=2))\n",
    "    frames = np.concatenate(frames, axis=0)[:,:,:,[2,1,0]]\n",
    "    return frames\n",
    "\n",
    "def view_video(frames, vpath):\n",
    "    if frames is None and Path(vpath).exists():\n",
    "        display(ipd.Video(vpath))\n",
    "        return\n",
    "        \n",
    "    vc = mpe.ImageSequenceClip(list(frames), fps=30)\n",
    "    ffmpeg_params = None\n",
    "    #ffmpeg_params=['-acodec', 'aac', '-preset', 'veryslow', '-crf', '17']\n",
    "    Path(vpath).parent.mkdir(parents=True, exist_ok=True)\n",
    "    vc.write_videofile(vpath, ffmpeg_params=ffmpeg_params, logger=None)\n",
    "    display(ipd.Video(vpath))\n",
    "\n",
    "def view_val(weight_path, vpath):\n",
    "    print(weight_path)\n",
    "    #dl = get_dl()\n",
    "    if not Path(vpath).exists():\n",
    "        model = load_model(weight_path)\n",
    "        frames = get_generated_frame(model, dl, device)\n",
    "    else:\n",
    "        frames = None\n",
    "    view_video(frames, vpath)\n",
    "\n",
    "def infer_model(PRJNAME, start=-1, end=-2, stride=-1):\n",
    "    weight_paths = sorted(glob(escape(f'weights/{PRJNAME}') + '/*.pth'))\n",
    "    for i, weight_path in enumerate(weight_paths[start:end:stride]):\n",
    "        view_val(weight_path, f'gen_videos/{PRJNAME}_{Path(weight_path).name}.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-behavior",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "PRJNAME = 'Adam_Grad_9_bs-144_lr-1e-05_mel_ps_80_2021-07-10 23:59'\n",
    "\n",
    "#while True:\n",
    "if True:\n",
    "    infer_model(PRJNAME, start=0,  end=100, stride=1)\n",
    "#    time.sleep(30*60)\n",
    "#    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-projection",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PRJNAME = 'Adam_Base_9_bs-144_lr-1e-05_mel_ps_80_2021-07-11 02:22'\n",
    "while True:\n",
    "    infer_model(PRJNAME, start=0,  end=100, stride=1)\n",
    "    time.sleep(30*60)\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-agenda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cna]",
   "language": "python",
   "name": "conda-env-cna-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
